{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a322eb-2631-4d7c-92be-6f0377f5341e",
   "metadata": {},
   "source": [
    "# develop get_remote_data_file for private s3 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38dac9a-b688-4d35-bd22-cb6e044bf1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "os.path.join(Path(torch.hub.get_dir()).parent, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192092b1-f340-414b-be71-9ac6fcaed529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the AWS profile\n",
    "os.environ['AWS_PROFILE'] = 'wasabi'\n",
    "\n",
    "# Set the AWS endpoint URL (optional)\n",
    "os.environ['S3_ENDPOINT_URL'] = 'https://s3.wasabisys.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5465b5-ec81-4784-a8e1-3f0e3755fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import errno\n",
    "import threading\n",
    "import hashlib\n",
    "import boto3\n",
    "import tarfile\n",
    "import zipfile\n",
    "from urllib.parse import urlparse\n",
    "from pathlib import Path\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "\n",
    "import torch\n",
    "from typing import Any, Callable, Dict, List, Mapping, Optional, Type, TypeVar, Union\n",
    "from torch.hub import download_url_to_file\n",
    "\n",
    "default_cache_dir = os.path.join(Path(torch.hub.get_dir()).parent, \"data\")\n",
    "\n",
    "def download_if_needed(url, cache_dir=default_cache_dir, progress=True, check_hash=False,\n",
    "                       delete_archive=True):\n",
    "    # Parse the URL to extract the filename\n",
    "    parsed_url = urlparse(url)\n",
    "    filename = os.path.basename(parsed_url.path)\n",
    "    \n",
    "    # Extract basename excluding file extensions\n",
    "    extensions = \"\".join(Path(filename).suffixes)\n",
    "    basename = Path(filename).name.replace(extensions, \"\")\n",
    "\n",
    "    # Determine the directory where the extracted contents will be stored\n",
    "    extract_dir = os.path.join(cache_dir, basename)\n",
    "\n",
    "    # Check if the directory already exists\n",
    "    if os.path.exists(extract_dir):\n",
    "        # The directory exists, no need to download and extract\n",
    "        return extract_dir\n",
    "\n",
    "    # Download the file as the directory does not exist    \n",
    "    downloaded_file = download_file(url, cache_dir, progress, check_hash)\n",
    "    \n",
    "    # extract archive\n",
    "    print(f\"Extracting file {downloaded_file} to folder {extract_dir}\")\n",
    "    extract_archive(downloaded_file, extract_dir, delete_archive=delete_archive, show_progress=progress)\n",
    "    \n",
    "    return extract_dir\n",
    "\n",
    "def extract_archive(archive_path, extract_to, delete_archive=True, show_progress=True, progress_interval=0.005):\n",
    "    if not os.path.exists(archive_path):\n",
    "        raise FileNotFoundError(f\"No archive file found at {archive_path}\")\n",
    "\n",
    "    def _report_progress(members, total):\n",
    "        last_reported = 0\n",
    "        interval_count = max(1, int(total * progress_interval))  # Calculate the interval count\n",
    "        for i, member in enumerate(members):\n",
    "            if show_progress and (i - last_reported >= interval_count or i == total - 1):\n",
    "                print(f\"Extracting file {i+1}/{total} ({(i+1)/total*100:.1f}%)\", end='\\r')\n",
    "                last_reported = i\n",
    "            yield member\n",
    "\n",
    "    # Determine the archive type and extract\n",
    "    if archive_path.endswith(('.tar', '.tar.gz', '.gz')):\n",
    "        with tarfile.open(archive_path, 'r:*') as archive:\n",
    "            members = archive.getmembers()\n",
    "            archive.extractall(path=extract_to, members=_report_progress(members, len(members)))\n",
    "    elif archive_path.endswith('.zip'):\n",
    "        with zipfile.ZipFile(archive_path, 'r') as archive:\n",
    "            members = archive.infolist()\n",
    "            for i, member in enumerate(_report_progress(members, len(members))):\n",
    "                archive.extract(member, path=extract_to)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported archive type for file {archive_path}\")\n",
    "\n",
    "    print()  # Print newline at end\n",
    "    # Optionally delete the archive after extraction\n",
    "    if delete_archive:\n",
    "        os.remove(archive_path)\n",
    "\n",
    "def download_file(url, cache_dir=default_cache_dir, progress=True, check_hash=True):\n",
    "    \n",
    "    if url.startswith(\"s3://\"):\n",
    "        cached_filename = download_from_s3(url, cache_dir=cache_dir, progress=progress, check_hash=check_hash)\n",
    "    elif url.startswith(\"http://\") or url.startswith(\"https://\"):\n",
    "        cached_filename = download_from_url(url, data_dir=cache_dir, progress=progress, check_hash=check_hash)\n",
    "    else:\n",
    "        raise ValueError(f\"URL should be a bucket object s3://<buckname>/<objectkey>, or valid web url http/https://example.com/<filename>: {url}\")\n",
    "    \n",
    "    return cached_filename\n",
    "\n",
    "def download_from_url(url: str, data_dir: Optional[str] = None, progress: bool = True, check_hash: bool = False,\n",
    "                      file_name: Optional[str] = None) -> Dict[str, Any]:\n",
    "    r\"\"\"Downloads the object at the given URL.\n",
    "\n",
    "    If downloaded file is a .tar file or .tar.gz file, it will be automatically\n",
    "    decompressed.\n",
    "\n",
    "    If the object is already present in `data_dir`, it's deserialized and\n",
    "    returned.\n",
    "    \n",
    "    The default value of ``data_dir`` is ``<hub_dir>/../data`` where\n",
    "    ``hub_dir`` is the directory returned by :func:`~torch.hub.get_dir`.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the object to download\n",
    "        data_dir (str, optional): directory in which to save the object\n",
    "        progress (bool, optional): whether or not to display a progress bar to stderr.\n",
    "            Default: True\n",
    "        check_hash(bool, optional): If True, the filename part of the URL should follow the naming convention\n",
    "            ``filename-<sha256>.ext`` where ``<sha256>`` is the first eight or more\n",
    "            digits of the SHA256 hash of the contents of the file. The hash is used to\n",
    "            ensure unique names and to verify the contents of the file.\n",
    "            Default: False\n",
    "        file_name (str, optional): name for the downloaded file. Filename from ``url`` will be used if not set.\n",
    "\n",
    "    Example:\n",
    "        >>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')\n",
    "\n",
    "    \"\"\"\n",
    "    # Issue warning to move data if old env is set\n",
    "    if os.getenv('TORCH_MODEL_ZOO'):\n",
    "        warnings.warn('TORCH_MODEL_ZOO is deprecated, please use env TORCH_HOME instead')\n",
    "    \n",
    "    if data_dir is None:\n",
    "        hub_dir = torch.hub.get_dir()\n",
    "        data_dir = default_cache_dir\n",
    "    \n",
    "    HASH_REGEX = re.compile(r'-([a-f0-9]{4,64})\\.')\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(data_dir)\n",
    "    except OSError as e:\n",
    "        if e.errno == errno.EEXIST:\n",
    "            # Directory already exists, ignore.\n",
    "            pass\n",
    "        else:\n",
    "            # Unexpected OSError, re-raise.\n",
    "            raise\n",
    "\n",
    "    parts = urlparse(url)\n",
    "    filename = os.path.basename(parts.path)\n",
    "    if file_name is not None:\n",
    "        filename = file_name\n",
    "    cached_file = os.path.join(data_dir, filename)\n",
    "    if not os.path.exists(cached_file):\n",
    "        sys.stderr.write('Downloading: \"{}\" to {}\\n'.format(url, cached_file))\n",
    "        hash_prefix = None\n",
    "        if check_hash:\n",
    "            #r = HASH_REGEX.search(filename)  # r is Optional[Match[str]]\n",
    "            #hash_prefix = r.group(1) if r else None\n",
    "            matches = HASH_REGEX.findall(filename) # matches is Optional[Match[str]]\n",
    "            hash_prefix = matches[-1] if matches else None\n",
    "\n",
    "        download_url_to_file(url, cached_file, hash_prefix, progress=progress)\n",
    "    \n",
    "    return cached_file\n",
    "\n",
    "def download_from_s3(s3_url, cache_dir=default_cache_dir, progress=True, check_hash=True) -> str:\n",
    "    \"\"\"\n",
    "    Download a file from an S3 bucket using AWS credentials and check its hash if required.\n",
    "\n",
    "    Args:\n",
    "        s3_url (str): S3 URL of the file to download (s3://bucket-name/path/to/file).\n",
    "        cache_dir (str): Directory where the file will be stored.\n",
    "        progress (bool): Whether to display download progress.\n",
    "        check_hash (bool): Whether to check the hash of the file.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the downloaded file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse the S3 URL\n",
    "    s3_bucket, s3_key = parse_s3_url(s3_url)\n",
    "\n",
    "    # Prepare the cache directory\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    cached_filename = os.path.join(cache_dir, os.path.basename(s3_key))\n",
    "\n",
    "    # Check if the file is already cached\n",
    "    if os.path.exists(cached_filename):\n",
    "        print(f\"File already exists: {cached_filename}\")\n",
    "        return cached_filename                    \n",
    "\n",
    "    # Explicitly get the endpoint URL and profile from the environment\n",
    "    endpoint_url = os.environ.get('S3_ENDPOINT_URL', 'https://s3.amazonaws.com')\n",
    "    profile_name = os.environ.get('AWS_PROFILE', 'default')\n",
    "\n",
    "    # Create a boto3 session with the specified profile\n",
    "    session = boto3.Session(profile_name=profile_name)\n",
    "\n",
    "    # Create an S3 client with the specified endpoint URL\n",
    "    s3 = session.client('s3', endpoint_url=endpoint_url)\n",
    "\n",
    "    try:\n",
    "        # Download the file from S3\n",
    "        print(f\"Downloading {s3_key} from bucket {s3_bucket}...\")\n",
    "        total_size = get_file_size(s3, s3_bucket, s3_key)\n",
    "        progress = DownloadProgressBar(cached_filename, total_size)\n",
    "        s3.download_file(s3_bucket, s3_key, cached_filename, Callback=progress)\n",
    "\n",
    "        if check_hash and not is_hash_matching(cached_filename, s3_key):\n",
    "            print(f\"Hash mismatch for file: {cached_filename}. Removing file from cache_dir...\")\n",
    "            os.remove(cached_filename)\n",
    "        #else:\n",
    "        #    print(f\"\\nDownloaded to {cached_filename}\")\n",
    "    except NoCredentialsError:\n",
    "        print(\"Error: AWS credentials not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "    return cached_filename\n",
    "\n",
    "def parse_s3_url(s3_url):\n",
    "    \"\"\"\n",
    "    Parse the S3 URL into bucket name and key.\n",
    "\n",
    "    Args:\n",
    "        s3_url (str): S3 URL of the file (s3://bucket-name/path/to/file).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the bucket name and the key.\n",
    "    \"\"\"\n",
    "    if not s3_url.startswith(\"s3://\"):\n",
    "        raise ValueError(\"URL must start with 's3://'\")\n",
    "\n",
    "    parts = s3_url[5:].split('/', 1)\n",
    "    if len(parts) < 2:\n",
    "        raise ValueError(\"URL must include both bucket name and key\")\n",
    "\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "def is_hash_matching(file_path, expected_hash):\n",
    "    \"\"\"\n",
    "    Check if the hash of the file at file_path matches the expected_hash.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file to check.\n",
    "        expected_hash (str): Expected hash value.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if hashes match, False otherwise.\n",
    "    \"\"\"\n",
    "    # HASH_REGEX = re.compile(r'-([a-f0-9]{8,})\\.')\n",
    "    HASH_REGEX = re.compile(r'-([a-f0-9]{4,64})\\.')\n",
    "    \n",
    "    # Extract expected hash from file_path\n",
    "    match = HASH_REGEX.search(file_path)\n",
    "    if not match:\n",
    "        print(\"No hash found in filename. Cannot verify hash.\")\n",
    "        return False\n",
    "\n",
    "    expected_hash = match.group(1)\n",
    "    actual_hash = calculate_file_hash(file_path)\n",
    "\n",
    "    return actual_hash.startswith(expected_hash)\n",
    "\n",
    "def calculate_file_hash(file_path):\n",
    "    \"\"\"\n",
    "    Calculate the SHA256 hash of a file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file.\n",
    "\n",
    "    Returns:\n",
    "        str: SHA256 hash of the file.\n",
    "    \"\"\"\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "            sha256_hash.update(byte_block)\n",
    "    return sha256_hash.hexdigest()\n",
    "\n",
    "def list_bucket(bucket_name, subfolder=None):\n",
    "    \"\"\"\n",
    "    List objects in an S3 bucket, optionally filtered by a subfolder.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): The name of the S3 bucket.\n",
    "        subfolder (str, optional): The subfolder path.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of object keys in the specified bucket and subfolder.\n",
    "    \"\"\"\n",
    "\n",
    "    # Explicitly get the endpoint URL and profile from the environment\n",
    "    endpoint_url = os.environ.get('S3_ENDPOINT_URL', 'https://s3.amazonaws.com')\n",
    "    profile_name = os.environ.get('AWS_PROFILE', 'default')\n",
    "\n",
    "    # Create a boto3 session with the specified profile\n",
    "    session = boto3.Session(profile_name=profile_name)\n",
    "\n",
    "    # Create an S3 client with the specified endpoint URL\n",
    "    s3 = session.client('s3', endpoint_url=endpoint_url)\n",
    "    object_keys = []\n",
    "\n",
    "    # If a subfolder is specified, ensure it ends with a '/'\n",
    "    if subfolder and not subfolder.endswith('/'):\n",
    "        subfolder += '/'\n",
    "\n",
    "    # List objects in the bucket\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=subfolder)\n",
    "\n",
    "    for page in page_iterator:\n",
    "        if \"Contents\" in page:\n",
    "            for obj in page['Contents']:\n",
    "                object_keys.append(obj['Key'])\n",
    "\n",
    "    return object_keys\n",
    "\n",
    "class DownloadProgressBar:\n",
    "    def __init__(self, filename, total_size, update_interval=.5):\n",
    "        self.filename = filename\n",
    "        self.total_size = total_size\n",
    "        self._seen_so_far = 0\n",
    "        self._last_reported_percentage = 0\n",
    "        self._update_interval = update_interval  # Update every 0.5%\n",
    "        self._lock = threading.Lock()\n",
    "\n",
    "    def __call__(self, bytes_amount):\n",
    "        with self._lock:\n",
    "            self._seen_so_far += bytes_amount\n",
    "            current_percentage = (self._seen_so_far / self.total_size) * 100\n",
    "\n",
    "            # Check if update is needed based on the interval\n",
    "            if (current_percentage - self._last_reported_percentage) >= self._update_interval or current_percentage >= 100:\n",
    "                sys.stdout.write(f\"\\r{self.filename}: {current_percentage:.2f}% downloaded\")\n",
    "                sys.stdout.flush()\n",
    "                self._last_reported_percentage = current_percentage\n",
    "\n",
    "    def finish(self):\n",
    "        # This method will be called when the download is complete\n",
    "        print(\"\")  # Print a newline character\n",
    "        \n",
    "def get_file_size(s3_client, bucket, key):\n",
    "    response = s3_client.head_object(Bucket=bucket, Key=key)\n",
    "    return response['ContentLength']\n",
    "\n",
    "def get_filename_without_suffixes(file_path):\n",
    "    extensions = \"\".join(Path(file_path).suffixes)\n",
    "    filename_without_suffixes = Path(file_path).name.replace(extensions, \"\")\n",
    "    return filename_without_suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e21c197-66d5-4fa1-9ea7-55cc292f874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'visionlab-datasets'\n",
    "subfolder = 'imagenet1k-256'\n",
    "objects = list_bucket(bucket_name, subfolder)\n",
    "print(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e8d653-6dcc-4ee5-b6e8-e0db00791b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '/n/holylabs/LABS/alvarez_lab/Users/alvarez/Sandbox/datasets/imagenet1k/val.tar.gz'\n",
    "file_name = '/n/holylabs/LABS/alvarez_lab/Users/alvarez/Sandbox/datasets/imagenet1k/ILSVRC2012_devkit_t12.tar.gz'\n",
    "hash_id = calculate_file_hash(file_name)[0:8]\n",
    "hash_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bcd3f1-e93d-4790-8054-807e2946294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello1?\"); print(\"\\n\"); print(\"hello2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223be8fa-fc5b-41f1-81ab-7fdfa09b236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '/n/holylabs/LABS/alvarez_lab/Users/alvarez/Sandbox/datasets/imagenet1k/val-d74759d1.tar.gz'\n",
    "is_hash_matching(file_name, hash_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf9cb19-c5b4-48a7-a3a5-942e912c5290",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm /n/alvarez_lab_tier1/Lab/cache/torch/data/val-d74759d1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73be77e6-dc01-498c-a14d-78a916dfa750",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = download_if_needed('s3://visionlab-datasets/imagenet1k-256/in1k-val-d74759d1.tar.gz')\n",
    "# data_dir = download_if_needed('https://s3.us-east-1.wasabisys.com/visionlab-datasets/imagenet1k-256/in1k-val-d74759d1.tar.gz')\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53e38a4-b9b3-44b0-b023-980409fd1626",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_file = download_file('s3://visionlab-datasets/imagenet1k-256/ILSVRC2012_devkit_t12.tar.gz',\n",
    "                            cache_dir=data_dir, check_hash=False)\n",
    "cached_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf562188-0d9d-4732-9908-0afdbc470c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageNet\n",
    "\n",
    "dataset = ImageNet(data_dir, split='val')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ad9d4-23d6-4d28-a775-a34fdc625701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b49fc30-6d3c-44de-b217-1e155f508b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "dataset = ImageFolder(data_dir)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1e6179-0655-4c12-91ed-8b0ac4c88a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dff73aa-dd18-47a8-b74e-903480fd9e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "cached_file = download_from_s3('s3://visionlab-datasets/imagenet1k-256/val-d74759d1.tar.gz', check_hash=True)\n",
    "cached_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4c2f4a-d056-4187-81ca-923e18f3a1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://s3.us-east-1.wasabisys.com/visionlab-datasets/imagenet1k-256/val-d74759d1.tar.gz'\n",
    "cached_file = download_from_url(url, progress=True, check_hash=True)\n",
    "cached_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c180ce04-d639-432b-9e33-bd2cfb6e04f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://s3.us-east-1.wasabisys.com/visionlab-datasets/imagenet1k-256/val-d74759d1.tar.gz'\n",
    "url = 's3://visionlab-datasets/imagenet1k-256/val-d74759d1.tar.gz'\n",
    "cached_file = download_file(url)\n",
    "cached_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84652fe0-74b3-4154-b0fc-9bbdd939c1e7",
   "metadata": {},
   "source": [
    "# imagenette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f0686c-5edc-4fa1-acc6-4929469c30d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/n/holylabs/LABS/alvarez_lab/Users/alvarez/Sandbox/datasets/imagenette/imagenette2-320.tgz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b16093-f072-48df-b8a0-479ec695b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_file_hash(filename)[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d74500b-3dad-4119-a2f5-1acc5b69e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_analytics.datasets import imagenette2_s320, imagenette2_s320_remap1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d59714-0ea1-40fb-b7f2-24810d4274c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = imagenette2_s320_remap1k(split='val')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebf8c02-5e58-42c3-b6dc-1bd2adfc9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "img,label,index = dataset[1000]\n",
    "print(label, index)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047e386e-4481-44bc-a522-8441eda088e8",
   "metadata": {},
   "source": [
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85ffa6d-9224-45af-a0a9-84b6f41f7c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_analytics.datasets import imagenet1k_s256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e2cd3d-16e8-49c1-af81-175bc3644b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = imagenet1k_s256('val')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e63d0b-07cd-47f1-b831-6ff78f344c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60585d66-bffe-4d41-b51f-973e02b2fd05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c8ce19-38cb-45b1-b409-a5b1a5ea44b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tarfile = '/n/alvarez_lab_tier1/Lab/cache/torch/data/val-d74759d1.tar.gz'\n",
    "cache_folder = get_filename_without_suffixes(tarfile)\n",
    "cache_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc312c-ab77-490b-999d-3c076332aed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TarImageFolder(tarfile, root_in_archive='val')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51613bc3-64b7-4871-b6aa-16b173342055",
   "metadata": {},
   "outputs": [],
   "source": [
    "img,label,index = dataset[10]\n",
    "print(label, index)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb34d83-b882-44b6-b70d-f8f83f16a718",
   "metadata": {},
   "outputs": [],
   "source": [
    "img,label,index = dataset[100]\n",
    "print(label, index)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e209d1-93fd-471c-978c-1eae9f3107f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img,label,index = dataset[1000]\n",
    "print(label, index)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbd5326-b6d4-439c-9996-32f306e9e822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
