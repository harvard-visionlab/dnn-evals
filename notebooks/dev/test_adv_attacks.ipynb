{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84a6c7e-02fc-406a-906f-44148b6e27f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3daabcd-611d-4d02-b7ff-5c5d549702ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_analytics.assays import AdversarialAttacks, AttackTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168b1176-db19-4a36-9484-a7543e6749eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "AttackTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f5c14b-1ddc-4e45-afd4-e10ad270eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "AdversarialAttacks.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f68d12-adb2-41cd-be13-4915dfbbd304",
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_adv_attack = AdversarialAttacks('imagenette2_s320_remap1k', split='val')\n",
    "assay_adv_attack.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3188104c-d7af-459e-8008-830804580da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img,label,index = assay_adv_attack.dataset[0]\n",
    "print(label,index,img.size)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6be452-bbe9-48b4-a5ca-87bc89c04422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models, transforms\n",
    "from functools import partial\n",
    "from model_rearing_workshop.models import load_model_from_weights\n",
    "from model_rearing_workshop.models.weights import get_standard_transforms, Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deca613-b8d0-4c68-b89f-9ee26b9c37ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "import contextlib\n",
    "import io\n",
    "import sys\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    new_stdout = io.StringIO()\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = new_stdout\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "        \n",
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__() \n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        embeddings, layer_outputs, layer_logits = self.model(x)\n",
    "        return embeddings\n",
    "    \n",
    "def load_model_eval(weights, model_name):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    with suppress_stdout():\n",
    "        model = load_model_from_weights(weights)\n",
    "    \n",
    "    wrapped_model = ModelWrapper(model)\n",
    "    wrapped_model.model_name = model_name\n",
    "    wrapped_model.to(device)\n",
    "    wrapped_model.eval()\n",
    "    \n",
    "    return wrapped_model\n",
    "\n",
    "def load_model_train(weights, model_name):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    with suppress_stdout():\n",
    "        model = load_model_from_weights(weights)\n",
    "    \n",
    "    wrapped_model = ModelWrapper(model)\n",
    "    wrapped_model.model_name = model_name\n",
    "    wrapped_model.to(device)\n",
    "    wrapped_model.train()\n",
    "    \n",
    "    return wrapped_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792c22d-a9c1-4035-9147-3024da705432",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = Weights(\n",
    "    url='https://visionlab-members.s3.wasabisys.com/alvarez/Projects/model_rearing_workshop/models/in1k/alexnet2023_layer_diffnoise_scaled_sqrt1/supervised/20231122_102040/final_weights-42b687fb09.pth',\n",
    "    transforms=get_standard_transforms(), # Add your transforms here\n",
    "    meta={\n",
    "        \"repo\": \"https://github.com/harvard-visionlab/alexnets\",\n",
    "        \"urls\": dict(\n",
    "            params='https://visionlab-members.s3.wasabisys.com/alvarez/Projects/model_rearing_workshop/models/in1k/alexnet2023_layer_diffnoise_scaled_sqrt1/supervised/20231122_102040/params-42b687fb09.json',\n",
    "            train='https://visionlab-members.s3.wasabisys.com/alvarez/Projects/model_rearing_workshop/models/in1k/alexnet2023_layer_diffnoise_scaled_sqrt1/supervised/20231122_102040/log_train-42b687fb09.txt',\n",
    "            val='https://visionlab-members.s3.wasabisys.com/alvarez/Projects/model_rearing_workshop/models/in1k/alexnet2023_layer_diffnoise_scaled_sqrt1/supervised/20231122_102040/log_val-42b687fb09.txt',\n",
    "        ),\n",
    "        \"_metrics\": {},\n",
    "        \"_docs\": \"\"\"\n",
    "            ....\n",
    "        \"\"\",\n",
    "    },\n",
    ")\n",
    "load_model = partial(load_model_eval, weights, 'alexnet2023_layer_diffnoise_scaled_sqrt1')\n",
    "transform = weights.transforms['val_transform']\n",
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7725594-1859-4b86-a4bc-a6bf03f11d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1 = assay_adv_attack.run(load_model, transform, attack=AttackTypes.FGSM)\n",
    "results_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c132f2-7b6a-4394-8f86-cca5e8e6a28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_adv_attack.plot_results(results_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328dfe44-14cf-4f27-a527-77076af9e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_train = assay_adv_attack.run(load_model_train, transform, attack=AttackTypes.FGSM)\n",
    "# results_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f34b4-543c-4f3c-b5a0-d27cbd73f471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assay_adv_attack.plot_results(results_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91347b18-c638-4d6b-afd5-ad456d457022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset1 = results_eval[results_eval.image_set=='adversarial']\n",
    "# subset1['mode'] = 'eval'\n",
    "# subset2 = results_train[results_train.image_set=='adversarial']\n",
    "# subset2['mode'] = 'train'\n",
    "\n",
    "# df = pd.concat([subset1, subset2])\n",
    "# assay_adv_attack.plot_results(df, hue=\"mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82da957-9a4b-4c61-9ac0-6db23f10184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = Weights(\n",
    "    url='https://visionlab-members.s3.wasabisys.com/alvarez/Projects/model_rearing_workshop/models/in1k/alexnet2023_inp_diffnoise_scaled_sqrt0/supervised/20231122_102051/final_weights-1d52da36f3.pth',\n",
    "    transforms=get_standard_transforms(), # Add your transforms here\n",
    "    meta={\n",
    "        \"repo\": \"https://github.com/harvard-visionlab/alexnets\",\n",
    "        \"urls\": dict(\n",
    "            params='https://visionlab-members.s3.wasabisys.com/alvarez/Projects/model_rearing_workshop/models/in1k/alexnet2023_inp_diffnoise_scaled_sqrt0/supervised/20231122_102051/params-1d52da36f3.json',\n",
    "            train='https://visionlab-members.s3.wasabisys.com/alvarez/Projects/model_rearing_workshop/models/in1k/alexnet2023_inp_diffnoise_scaled_sqrt0/supervised/20231122_102051/log_train-1d52da36f3.txt',\n",
    "            val='https://visionlab-members.s3.wasabisys.com/alvarez/Projects/model_rearing_workshop/models/in1k/alexnet2023_inp_diffnoise_scaled_sqrt0/supervised/20231122_102051/log_val-1d52da36f3.txt',\n",
    "        ),\n",
    "        \"_metrics\": {},\n",
    "        \"_docs\": \"\"\"\n",
    "            ....\n",
    "        \"\"\",\n",
    "    },\n",
    ")\n",
    "load_model = partial(load_model_eval, weights, 'alexnet2023_inp_diffnoise_scaled_sqrt0')\n",
    "transform = weights.transforms['val_transform']\n",
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2378bdfa-ac52-4bc8-9881-c9dc660c20f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2 = assay_adv_attack.run(load_model, transform, attack=AttackTypes.FGSM)\n",
    "results_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957b8ce5-984c-40e4-9ab1-ef682d203f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_adv_attack.plot_results(results_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b53902f-bfc6-4384-8cdf-76de1b969f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_WEIGHTS = Weights(\n",
    "    url='https://visionlab-members.s3.wasabisys.com/alvarez/Projects/model_rearing_workshop/models/in1k/alexnet2023_inp_diffnoise_scaled_sqrt1/supervised/20231122_041727/final_weights-1e96f04a88.pth',\n",
    "    transforms=get_standard_transforms(), # Add your transforms here\n",
    "    meta={\n",
    "        \"repo\": \"https://github.com/harvard-visionlab/alexnets\",\n",
    "        \"urls\": dict(\n",
    "            params='https://visionlab-members.s3.wasabisys.com/alvarez/Projects/model_rearing_workshop/models/in1k/alexnet2023_inp_diffnoise_scaled_sqrt1/supervised/20231122_041727/params-1e96f04a88.json',\n",
    "            train='https://visionlab-members.s3.wasabisys.com/alvarez/Projects/model_rearing_workshop/models/in1k/alexnet2023_inp_diffnoise_scaled_sqrt1/supervised/20231122_041727/log_train-1e96f04a88.txt',\n",
    "            val='https://visionlab-members.s3.wasabisys.com/alvarez/Projects/model_rearing_workshop/models/in1k/alexnet2023_inp_diffnoise_scaled_sqrt1/supervised/20231122_041727/log_val-1e96f04a88.txt',\n",
    "        ),\n",
    "        \"_metrics\": {},\n",
    "        \"_docs\": \"\"\"\n",
    "            ....\n",
    "        \"\"\",\n",
    "    },\n",
    ")\n",
    "load_model = partial(load_model_eval, weights, 'alexnet2023_inp_diffnoise_scaled_sqrt1')\n",
    "transform = weights.transforms['val_transform']\n",
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e505e5-7628-4684-abca-06b7757a9ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_3 = assay_adv_attack.run(load_model, transform, attack=AttackTypes.FGSM)\n",
    "results_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304035d0-c329-47a6-b088-c8b29c1f334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = Weights(\n",
    "#     url='https://visionlab-members.s3.wasabisys.com/alvarez/Projects/model_rearing_workshop/models/in1k/alexnet2023_baseline/barlow/20231116_044210/final_weights-0b70b9da61.pth',\n",
    "#     transforms=get_standard_transforms(), # Add your transforms here\n",
    "#     meta={\n",
    "#         \"repo\": \"https://github.com/harvard-visionlab/alexnets\",\n",
    "#         \"urls\": dict(\n",
    "#             params='https://visionlab-members.s3.wasabisys.com/alvarez/Projects/model_rearing_workshop/models/in1k/alexnet2023_baseline/barlow/20231116_044210/params-0b70b9da61.json',\n",
    "#             train='https://visionlab-members.s3.wasabisys.com/alvarez/Projects/model_rearing_workshop/models/in1k/alexnet2023_baseline/barlow/20231116_044210/log_train-0b70b9da61.txt',\n",
    "#             val='https://visionlab-members.s3.wasabisys.com/alvarez/Projects/model_rearing_workshop/models/in1k/alexnet2023_baseline/barlow/20231116_044210/log_val-0b70b9da61.txt',\n",
    "#         ),\n",
    "#         \"_metrics\": {},\n",
    "#         \"_docs\": \"\"\"\n",
    "#             ....\n",
    "#         \"\"\",\n",
    "#     },\n",
    "# )\n",
    "\n",
    "weights = Weights(\n",
    "    url='https://visionlab-members.s3.wasabisys.com/alvarez/Projects/model_rearing_workshop/models/in1k/alexnet2023_baseline/supervised/20231115_062107/final_weights-b0b1f89b7a.pth',\n",
    "    transforms=get_standard_transforms(), # Add your transforms here\n",
    "    meta={\n",
    "        \"repo\": \"https://github.com/harvard-visionlab/alexnets\",\n",
    "        \"urls\": dict(\n",
    "            params='https://visionlab-members.s3.wasabisys.com/alvarez/Projects/model_rearing_workshop/models/in1k/alexnet2023_baseline/supervised/20231115_062107/params-b0b1f89b7a.json',\n",
    "            train='https://visionlab-members.s3.wasabisys.com/alvarez/Projects/model_rearing_workshop/models/in1k/alexnet2023_baseline/supervised/20231115_062107/log_train-b0b1f89b7a.txt',\n",
    "            val='https://visionlab-members.s3.wasabisys.com/alvarez/Projects/model_rearing_workshop/models/in1k/alexnet2023_baseline/supervised/20231115_062107/log_val-b0b1f89b7a.txt',\n",
    "        ),\n",
    "        \"_metrics\": {},\n",
    "        \"_docs\": \"\"\"\n",
    "            ....\n",
    "        \"\"\",\n",
    "    },\n",
    ")\n",
    "\n",
    "load_model = partial(load_model_eval, weights, 'alexnet2023_baseline')\n",
    "transform = weights.transforms['val_transform']\n",
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6c53d9-3f9b-4df7-a9b1-9db28073e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_baseline = assay_adv_attack.run(load_model, transform, attack=AttackTypes.FGSM)\n",
    "results_baseline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d503ee1c-ab32-4878-add0-b528699a664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_adv_attack.plot_results(results_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a4004a-8db1-45f1-8156-8288a4a1c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_baseline.to_csv(\"./results/testing.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bfa72e-ec26-4f7b-87d6-b7deeab4781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# results_baseline = pd.read_csv(\"./results/testing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef9fc57-13f2-4b08-b4c8-0be8c24f656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = results_baseline[results_baseline.image_set=='adversarial']\n",
    "df2 = results_1[results_1.image_set=='adversarial']\n",
    "df3 = results_2[results_2.image_set=='adversarial']\n",
    "df4 = results_3[results_2.image_set=='adversarial']\n",
    "df = pd.concat([df1, df2, df3, df4])\n",
    "\n",
    "model_names = list(df.model_name.unique())\n",
    "image_sets = list(df.image_set.unique())\n",
    "epsilons = sorted(list(df.epsilon.unique()))\n",
    "item_names = list(df.filenames.unique())\n",
    "\n",
    "model_names, image_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd35cff-dba3-48f0-a1a4-e00766dbc418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deep_analytics.assays.adversarial_attacks.metrics import dataframe_to_array\n",
    "\n",
    "# data_column = 'correct1'\n",
    "# results = dataframe_to_array(df[df.image_set=='adversarial'], data_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7530f76d-9711-4180-8bcb-662eca80f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = assay_adv_attack.compute_metrics(df, 'correct1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd369c7d-afdf-44af-a23c-f694af566d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics['acc_by_eps'].stats()['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a24f50d-08ec-4ed3-a4c4-8771e59d27d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics['norm_acc_by_eps'].stats()['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d2f5e1-217a-4b82-9ba6-6c8b11d7e0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics['norm_acc_by_eps'].stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eaee95-619f-4ece-bd00-18ac2f259506",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics['norm_auc'].stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d358af6-a7a4-48b4-ad35-37b1e29cba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics['norm_acc'].stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0583112c-3dc8-44fd-91f0-0fcd4ff23435",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics['norm_adv_acc'].stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12039ab2-b884-46b8-9a09-d74cf5e394a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics['weighted_norm_adv_acc'].stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e604d584-2c67-442c-8751-301fbaa8c7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics['thresh_half_minmax'].stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379a8e59-538b-467a-b60b-7ba62019b8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2430a7-3450-4c03-b47d-63d56eb86319",
   "metadata": {},
   "outputs": [],
   "source": [
    "means[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e845093c-7a3b-4b49-9c63-7d6aecc00790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "stats = metrics['norm_acc_by_eps'].stats()\n",
    "means = stats['mean']\n",
    "lower, upper = stats['95% CI']\n",
    "means.shape, lower.shape, upper.shape\n",
    "\n",
    "ax = sns.lineplot(x=epsilons, y=means[0,0], label=model_names[0])\n",
    "ax = sns.lineplot(x=epsilons, y=means[1,0], label=model_names[1], ax=ax)\n",
    "ax = sns.lineplot(x=epsilons, y=means[2,0], label=model_names[2], ax=ax)\n",
    "ax = sns.lineplot(x=epsilons, y=means[3,0], label=model_names[3], ax=ax)\n",
    "ax.set_title('normalized acc vs. epsilon')\n",
    "ax.set_ylabel('normalized accuracy')\n",
    "ax.set_xlabel('epsilon');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb23bd8d-63a0-4e4d-a63c-c389d5565563",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = metrics['acc_by_eps'].stats()\n",
    "means = stats['mean']\n",
    "lower, upper = stats['95% CI']\n",
    "means.shape, lower.shape, upper.shape\n",
    "\n",
    "ax = sns.lineplot(x=epsilons, y=means[0,0], label=model_names[0])\n",
    "ax = sns.lineplot(x=epsilons, y=means[1,0], label=model_names[1], ax=ax)\n",
    "ax = sns.lineplot(x=epsilons, y=means[2,0], label=model_names[2], ax=ax)\n",
    "ax = sns.lineplot(x=epsilons, y=means[3,0], label=model_names[3], ax=ax)\n",
    "ax.set_title('acc vs. epsilon')\n",
    "ax.set_ylabel('accuracy')\n",
    "ax.set_xlabel('epsilon');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c6917b-32e7-486a-bf24-0a9af5ba99c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "scores = np.array(metrics['norm_adv_acc'].scores)\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca78534c-6aaf-4f73-89cc-c171c0f1a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = scores[:,0,0]\n",
    "b = scores[:,1,0]\n",
    "\n",
    "a = scores[:,2,0]\n",
    "b = scores[:,3,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc2a8e5-5440-411c-a7af-64d770271a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = b-a\n",
    "g = sns.displot(delta)\n",
    "g.set(xlim=(-max(abs(delta))*1.1, max(abs(delta))*1.1));\n",
    "alpha = .05\n",
    "\n",
    "# compute the confidence interval\n",
    "lower_bound, upper_bound = np.percentile(delta, [alpha*100/2, 100-alpha*100/2])\n",
    "\n",
    "# if zero is not within the confidence interval, there's a reliable difference at alpha cutoff\n",
    "significant = not (lower_bound <= 0 <= upper_bound)\n",
    "significant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23ced55-ed8d-4782-b648-838a86bdcb4a",
   "metadata": {},
   "source": [
    "# Quantify Robustness\n",
    "\n",
    "1. convert pandas dataframe to numpy array\n",
    "2. compute mean and upper/lower CI for each image_set + epsilon combo\n",
    "3. Compute robustness scores\n",
    "- [ ] normalized AUC sum(acc_by_eps / acc_eps_0)\n",
    "- [ ] normalized acc mean(acc_by_eps/acc_eps_0)\n",
    "- [ ] compute equal-weighted robust accuracy: \n",
    "    weights = [1.0] * len(epsilons)  # Equal weights\n",
    "    robust_accuracy = clean_accuracy - np.average(adversarial_accuracies, weights=weights)\n",
    "- [ ] compute weighted robust accuracy\n",
    "    weights = 1/epsilons[1:] # Inverse proportional weights\n",
    "    robust_accuracy = clean_accuracy - np.average(adversarial_accuracies, weights=weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b412e3-cce6-482f-8299-a2eb99ae5f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_analytics.assays.adversarial_attacks.adversarial_attacks import dataframe_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a28a47-a670-4cc4-aa28-4ff87491fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = dataframe_to_array(results_baseline[results_baseline.image_set=='adversarial'], 'correct1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24f174f-726f-4373-bf69-9b6d45cbaa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(arr.keys())\n",
    "print(arr['dims'])\n",
    "D = arr['D']\n",
    "epsilons = np.array(arr['epsilons'])\n",
    "D.shape, epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb67029-a5b3-4f10-9afc-bb809ebaa9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59553a13-5af6-4a98-beb1-c7aac2ecacb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "from fastprogress import progress_bar \n",
    "\n",
    "class AccumMetric:\n",
    "    def __init__(self, scoring_func, ci_level=0.95):\n",
    "        self.scores = []\n",
    "        self.scoring_func = scoring_func\n",
    "        self.ci_level = ci_level    \n",
    "\n",
    "    def reset(self):\n",
    "        self.scores = []\n",
    "\n",
    "    def stats(self, ci_level=None, axis=None):\n",
    "        if not self.scores:\n",
    "            return None\n",
    "        ci_level = self.ci_level if ci_level is None else ci_level\n",
    "        axis = 0 if axis is None else axis\n",
    "        \n",
    "        # Calculate the mean\n",
    "        mean_score = np.mean(self.scores, axis=axis)\n",
    "\n",
    "        # Calculate the lower and upper percentiles for the confidence interval\n",
    "        lower_percentile = 100 * (1 - self.ci_level) / 2\n",
    "        upper_percentile = 100 * (1 + self.ci_level) / 2\n",
    "\n",
    "        lower_ci = np.percentile(self.scores, lower_percentile, axis=axis)\n",
    "        upper_ci = np.percentile(self.scores, upper_percentile, axis=axis)\n",
    "\n",
    "        return {\n",
    "            \"mean\": mean_score,\n",
    "            f\"{int(self.ci_level * 100)}% CI\": (lower_ci, upper_ci),\n",
    "        }\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        # Calculate the score using the provided scoring function\n",
    "        score = self.scoring_func(data)\n",
    "        self.scores.append(score)\n",
    "        \n",
    "def estimate_thresh_crossing(xs, ys, threshold):\n",
    "    crossings = []\n",
    "\n",
    "    for i in range(len(xs) - 1):\n",
    "        x1, x2 = xs[i], xs[i + 1]\n",
    "        y1, y2 = ys[i], ys[i + 1]\n",
    "\n",
    "        if (y1 < threshold and y2 >= threshold) or (y1 >= threshold and y2 < threshold):\n",
    "            # Linear interpolation to estimate the crossing point\n",
    "            slope = (y2 - y1) / (x2 - x1)\n",
    "            intercept = y1 - slope * x1\n",
    "            crossing_x = (threshold - intercept) / slope\n",
    "            crossings.append(crossing_x)\n",
    "\n",
    "    if crossings:\n",
    "        # Calculate the average crossing point if multiple crossings occurred\n",
    "        estimated_epsilon = np.mean(crossings)\n",
    "    else:\n",
    "        estimated_epsilon = None\n",
    "\n",
    "    return estimated_epsilon, crossings\n",
    "\n",
    "# def compute_normalized_acc_by_eps(data, epsilons, dim=-1):\n",
    "#     acc_by_eps = data.mean(axis=dim)\n",
    "#     return acc_by_eps\n",
    "\n",
    "# def compute_normalized_auc(data, epsilons, dim=-1):\n",
    "#     assert epsilons[0]==0, f\"oops, expected first epsilon to be zero, got {epsilons[0]}\"\n",
    "#     acc_by_eps = data.mean(axis=dim)\n",
    "#     clean_accuracy = acc_by_eps[0]\n",
    "#     normed_acc = acc_by_eps / clean_accuracy\n",
    "#     normalized_auc = np.sum(normed_acc)\n",
    "#     return normalized_auc\n",
    "\n",
    "# def compute_normalized_acc(data, epsilons, dim=-1):\n",
    "#     assert epsilons[0]==0, f\"oops, expected first epsilon to be zero, got {epsilons[0]}\"\n",
    "#     acc_by_eps = data.mean(axis=dim)\n",
    "#     clean_accuracy = acc_by_eps[0]\n",
    "#     normed_acc = acc_by_eps / clean_accuracy\n",
    "#     normalized_acc = np.mean(normed_acc)\n",
    "#     return normalized_acc\n",
    "\n",
    "# def compute_weighted_adv_acc(data, epsilons, dim=-1, normalize=True):\n",
    "#     assert epsilons[0]==0, f\"oops, expected first epsilon to be zero, got {epsilons[0]}\"\n",
    "#     acc_by_eps = data.mean(axis=dim)\n",
    "#     clean_accuracy = acc_by_eps[0]\n",
    "#     adv_accuracies = acc_by_eps[1:]\n",
    "#     if normalize:\n",
    "#         adv_accuracies = adv_accuracies / clean_accuracy\n",
    "#     weighted_adv_acc = np.average(adv_accuracies, weights=1/epsilons[1:])\n",
    "#     return weighted_adv_acc\n",
    "\n",
    "# def compute_thresh_half_minmax(data, epsilons, dim=-1, normalize=True):\n",
    "#     assert epsilons[0]==0, f\"oops, expected first epsilon to be zero, got {epsilons[0]}\"\n",
    "#     acc_by_eps = data.mean(axis=dim)\n",
    "#     clean_accuracy = acc_by_eps[0]\n",
    "#     normed_acc = acc_by_eps / clean_accuracy\n",
    "#     half_max = normed_acc.min() + (normed_acc.max() - normed_acc.min())/2\n",
    "#     thresh_half_minmax, _ = estimate_thresh_crossing(epsilons, normed_acc, half_max)\n",
    "    \n",
    "#     return thresh_half_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c5f86-5b53-4491-937e-a373da6c2c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_analytics.utils.bootstrap import bootstrap_multi_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ffccb2-301f-4d3b-b7f5-a7472d596cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samples = bootstrap_multi_dim(D[0,0], dims=(1), n_bootstrap=1000, seed=1234)\n",
    "\n",
    "metrics = [\n",
    "    AccumMetric(partial(compute_normalized_acc_by_eps, epsilons=epsilons)),\n",
    "    AccumMetric(partial(compute_normalized_auc, epsilons=epsilons)),\n",
    "    AccumMetric(partial(compute_normalized_acc, epsilons=epsilons)),\n",
    "    AccumMetric(partial(compute_weighted_adv_acc, epsilons=epsilons, normalize=True)),\n",
    "    AccumMetric(partial(compute_thresh_half_minmax, epsilons=epsilons, normalize=True))\n",
    "]\n",
    "\n",
    "for sample in progress_bar(samples):\n",
    "    for metric in metrics: \n",
    "        metric(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7905b003-463d-42a5-b869-f1ad81501969",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = dict([\n",
    "    ('acc_by_eps', AccumMetric(partial(compute_normalized_acc_by_eps, epsilons=epsilons))),\n",
    "    ('norm_auc', AccumMetric(partial(compute_normalized_auc, epsilons=epsilons))),\n",
    "    ('norm_acc', AccumMetric(partial(compute_normalized_acc, epsilons=epsilons))),\n",
    "    ('weighted_norm_acc', AccumMetric(partial(compute_weighted_adv_acc, epsilons=epsilons, normalize=True))),\n",
    "    ('thresh_half_minmax', AccumMetric(partial(compute_thresh_half_minmax, epsilons=epsilons, normalize=True)))\n",
    "])\n",
    "metrics.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0faba1-efed-4289-a0e7-94a7f31a04a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(metrics[0].scores).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb7aa95-3782-408c-897c-df7310181ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[0].stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb5a7e9-43ad-435a-a907-bbecd04eec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[1].stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39da1e1d-c44a-4657-be3a-3f37211f8730",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[2].stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b51a17f-ba7a-48ac-b669-4b54c216074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[3].stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60850d6-b5ad-45ab-8bb2-d47dbf837830",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[4].stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ec83f3-1baa-4ed0-9b62-e41ee34b44f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert epsilons[0]==0\n",
    "acc_by_eps = D[0,0].mean(axis=1)\n",
    "clean_accuracy = acc_by_eps[0]\n",
    "adv_accuracies = acc_by_eps[1:]\n",
    "normed_acc = acc_by_eps / clean_accuracy\n",
    "normalized_auc = np.sum(normed_acc)\n",
    "normalized_acc = np.mean(normed_acc)\n",
    "weighted_adv_acc = np.average(adv_accuracies, weights=1/epsilons[1:])\n",
    "half_max = normed_acc.min() + (normed_acc.max() - normed_acc.min())/2\n",
    "thresh_half_minmax, _ = estimate_thresh_crossing(epsilons, normed_acc, half_max)\n",
    "\n",
    "normalized_auc, normalized_acc, weighted_adv_acc, thresh_half_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c422359-917e-439c-9be2-76cacda603ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7eda8d-f3cc-450b-a73f-4670c621ad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_by_eps, epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03dbd0-b907-4e27-8bb2-4893be77475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_epsilon_crossing(epsilons, acc_by_eps, 1/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d5e3ff-ecb4-4949-8162-34b6d2f8cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# from numpy.random import RandomState\n",
    "# from fastprogress import progress_bar \n",
    "\n",
    "# def dataframe_to_array(df, data_column):\n",
    "#     '''\n",
    "#         convert dataframe into a numModels x numImageSets x numEpsilons x numItems array\n",
    "#     '''\n",
    "#     model_names = list(df.model_name.unique())\n",
    "#     image_sets = list(df.image_set.unique())\n",
    "#     epsilons = sorted(list(df.epsilon.unique()))\n",
    "#     item_names = list(df.filenames.unique())\n",
    "    \n",
    "#     D = np.empty((len(model_names),len(image_sets),len(epsilons),len(item_names)))\n",
    "#     D[:] = np.nan\n",
    "#     for rownum,row in progress_bar(df.iterrows(), total=len(df)):\n",
    "#         model_num = model_names.index(row.model_name)\n",
    "#         imageset_num = image_sets.index(row.image_set)\n",
    "#         epsilon_num = epsilons.index(row.epsilon)\n",
    "#         item_num = item_names.index(row.filenames)\n",
    "#         curr_val = D[model_num, imageset_num, epsilon_num, item_num]\n",
    "#         assert np.isnan(curr_val), f\"Oops, expected current value to be nan, got {curr_val}\"    \n",
    "#         D[model_num, imageset_num, epsilon_num, item_num] = row[data_column]\n",
    "#     assert np.isnan(D).any() == False, \"Oops, expected all values to be filled, found nans\"    \n",
    "    \n",
    "#     return dict(\n",
    "#         D=D,\n",
    "#         dims=['model_name', 'image_set', 'epsilon', 'filename'],\n",
    "#         model_names=model_names,\n",
    "#         image_sets=image_sets,\n",
    "#         epsilons=epsilons,\n",
    "#         item_names=item_names\n",
    "#     )\n",
    "\n",
    "# def bootstrap_single_dim(D, dim, n_bootstrap=1000, seed=123):\n",
    "#     num_items = D.shape[dim]\n",
    "    \n",
    "#     # Initialize the random number generator\n",
    "#     rng = RandomState(seed)\n",
    "    \n",
    "#     # Generate bootstrap samples\n",
    "#     samples = rng.choice(num_items, size=(n_bootstrap, num_items), replace=True)\n",
    "\n",
    "#     # Select the samples along the specified dimension\n",
    "#     bootstrap_samples = np.take(D, samples, axis=dim)\n",
    "    \n",
    "#     # Move the bootstrap dimension to the first position\n",
    "#     bootstrap_samples = np.moveaxis(bootstrap_samples, dim, 0)\n",
    "    \n",
    "#     return bootstrap_samples\n",
    "\n",
    "# def bootstrap_multi_dim_slow_loop_test(D, dims, n_bootstrap=10000, seed=123):\n",
    "#     rng = RandomState(seed)\n",
    "\n",
    "#     # Initialize the bootstrap sample array\n",
    "#     new_shape = list(D.shape) + [n_bootstrap]\n",
    "#     bootstrap_samples = np.empty(new_shape, dtype=D.dtype)\n",
    "\n",
    "#     for i in range(n_bootstrap):\n",
    "#         # Initialize indices for this bootstrap sample\n",
    "#         indices = [slice(None)] * D.ndim\n",
    "\n",
    "#         # Replace indices for the bootstrapped dimensions\n",
    "#         for dim in dims:\n",
    "#             num_items = D.shape[dim]\n",
    "#             indices[dim] = rng.choice(num_items, size=num_items, replace=True)\n",
    "\n",
    "#         # Using advanced indexing to select elements for the bootstrap sample\n",
    "#         bootstrap_samples[..., i] = D[np.ix_(*indices)]\n",
    "\n",
    "#     return bootstrap_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd084e92-bacc-40c7-b97a-91d6920c9543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df.cond_name.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2ae9d4-c7dc-4827-a22f-d096e073da57",
   "metadata": {},
   "source": [
    "# figure out multi-dimensionsal bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c598cc09-fb0a-48b9-8faf-c12700a6279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# from numpy.random import RandomState\n",
    "\n",
    "# def generate_test_data(num_subjects=3, num_conds=2, num_items=5):\n",
    "#     D = np.zeros((num_subjects, num_conds, num_items))\n",
    "\n",
    "#     # Populate the array with unique values for each subject and each item\n",
    "#     for subject in range(num_subjects):\n",
    "#         for cond in range(num_conds):\n",
    "#             # Offset each condition by 100\n",
    "#             D[subject, cond, :] = np.arange(subject * 100, subject * 100 + num_items) + (cond * 100)\n",
    "    \n",
    "#     return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c530787d-ac53-4458-9bfe-fd19f1351f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "def bootstrap_multi_dim(D, dims, n_bootstrap=10000, seed=None, vectorized=False):\n",
    "    \"\"\"\n",
    "    Perform bootstrap resampling across specified dimensions of a multi-dimensional array.\n",
    "\n",
    "    Args:\n",
    "    D (numpy.ndarray): The input array from which to sample.\n",
    "    dims (int or tuple of ints): The dimensions over which to perform bootstrapping.\n",
    "    n_bootstrap (int): The number of bootstrap samples to generate.\n",
    "    seed (int): Random seed for reproducibility.\n",
    "    vectorized (bool): Whether to use vectorized indexing (flatten data, use flat_indices)\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: An array of bootstrapped samples. The shape of the array is \n",
    "                   (n_bootstrap, *D.shape), where the first dimension corresponds\n",
    "                   to the bootstrap samples, and the remaining dimensions correspond\n",
    "                   to the dimensions of the original array.\n",
    "\n",
    "    The function generates random indices for the specified dimensions (dims) and \n",
    "    uses the original indices for the other dimensions. These indices are expanded \n",
    "    and scaled by the array's strides to calculate the flat indices, which are then \n",
    "    used to index into a flattened version of the original array. The resulting \n",
    "    samples are reshaped to form an array that retains the structure of the original \n",
    "    array while incorporating the bootstrap dimension.\n",
    "    \"\"\"    \n",
    "    if isinstance(dims, int): dims = (dims,)\n",
    "\n",
    "    rng = RandomState(seed)\n",
    "\n",
    "    # Generate random indices for each specified dimension, repeat original indices for other dimensions\n",
    "    indices = [rng.choice(D.shape[dim], size=(n_bootstrap, D.shape[dim]), replace=True) \n",
    "               if dim in dims else np.arange(D.shape[dim])[None,:].repeat(n_bootstrap, 0)\n",
    "               for dim in range(D.ndim)]\n",
    "\n",
    "    if vectorized:\n",
    "        bootstrap_samples = vectorized_indexing(D, indices)\n",
    "    else:\n",
    "        bootstrap_samples = loop_indexing(D, indices)\n",
    "    \n",
    "    return bootstrap_samples\n",
    "\n",
    "def loop_indexing(D, indices):\n",
    "    \n",
    "    n_bootstrap = indices[0].shape[0]\n",
    "    \n",
    "    # Initialize the bootstrap sample array\n",
    "    new_shape = [n_bootstrap] + list(D.shape)\n",
    "    bootstrap_samples = np.empty(new_shape, dtype=D.dtype)\n",
    "\n",
    "    # Iterate over the indices for each bootstrap sample\n",
    "    for i, curr_indices in enumerate(progress_bar(zip(*indices), total=n_bootstrap)):\n",
    "        # Use advanced indexing to select the sample\n",
    "        bootstrap_samples[i] = D[np.ix_(*curr_indices)]\n",
    "    \n",
    "    # Now, bootstrap_samples contains the bootstrapped samples    \n",
    "    return bootstrap_samples\n",
    "\n",
    "def vectorized_indexing(D, indices):\n",
    "    n_bootstrap = indices[0].shape[0]\n",
    "    \n",
    "    # Calculate the strides for each dimension in D\n",
    "    strides = np.array(D.strides) // D.itemsize\n",
    "\n",
    "    # Flatten the array D\n",
    "    D_flat = D.reshape(-1)\n",
    "    \n",
    "    # lambda function to expand dimensions on indices so they broadcast correctly when summed\n",
    "    # e.g., if D is 5x2x100, n_bootstrap=1000, then reshape indices dim=0 to be 1000x5x1x1, etc.\n",
    "    adjusted_axes = lambda dim: [i+1 for i in range(D.ndim) if i != dim]\n",
    "    \n",
    "    # Expand Dims and Multiply each set of indices with the corresponding stride before summing\n",
    "    scaled_indices = [np.ascontiguousarray(np.expand_dims(idx, axis=adjusted_axes(dim)) * stride).astype(np.int32)\n",
    "                      for dim, (idx, stride) in enumerate(zip(indices, strides))]\n",
    "\n",
    "    # Perform the direct summation using reduce (np.add performs the correct broadcasting)\n",
    "    flat_indices = reduce(np.add, scaled_indices)\n",
    "    \n",
    "    # Use the flat indices to access the data values\n",
    "    bootstrap_samples_flat = D_flat[flat_indices]\n",
    "    \n",
    "    # reshape \n",
    "    bootstrap_samples = bootstrap_samples_flat.reshape(n_bootstrap, *D.shape)\n",
    "    \n",
    "    return bootstrap_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4f85d-d4ed-498a-be56-876ea2d4206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D = generate_test_data()\n",
    "# print(\"Original Data:\")\n",
    "# print(D)\n",
    "# print(D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea2c7af-b80d-471a-95a0-700561af0eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D = torch.rand((10,2,1260)).numpy()\n",
    "# D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6c7778-4b5a-4031-86cb-78541dd2eec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# start = time.time()\n",
    "# bs_samples1 = bootstrap_multi_dim(D, dims=(0,2), seed=123, vectorized=False)\n",
    "# dur = time.time() - start\n",
    "# bs_samples1.shape, dur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557bb9b5-dc92-49fd-b947-daefd2830593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# start = time.time()\n",
    "# bs_samples2 = bootstrap_multi_dim(D, dims=(0,2), seed=123, vectorized=True)\n",
    "# dur = time.time() - start\n",
    "# bs_samples2.shape, dur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5956f4df-8279-4ec8-865a-286fe30f1872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.equal(bs_samples1, bs_samples2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b6b17a-29a7-4239-a86d-24074b2e3130",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
