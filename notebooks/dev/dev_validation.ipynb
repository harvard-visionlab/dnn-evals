{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74c6d8ed-f36f-4e02-9d43-96a8092fab03",
   "metadata": {},
   "source": [
    "# let's validate\n",
    "\n",
    "So you trained a model and want to test your classification accuracy?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0071667c-77f1-4343-b23e-9fa7f0aa31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343420a3-51c4-4f58-b5aa-7d00328c812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from contextlib import redirect_stdout\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchattacks\n",
    "import matplotlib.pyplot as plt\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from deep_analytics.assays.model_assay import ModelAssay\n",
    "from deep_analytics.utils.bootstrap import bootstrap_multi_dim\n",
    "from deep_analytics.utils.stats import AccumMetric\n",
    "\n",
    "# from deep_analytics.assays.metrics import *\n",
    "\n",
    "from pdb import set_trace\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "__all__ = ['ClassificationAccuracy']\n",
    "\n",
    "class ClassificationAccuracy(ModelAssay):\n",
    "    \n",
    "    datasets = dict(\n",
    "        imagenette2=('imagenette2_s320_remap1k', 'val'),\n",
    "        imagenet1k=('imagenet1k_s256', 'val'),\n",
    "        imagenetV2_top_images=('imagenetV2', 'top-images'),\n",
    "        imagenetV2_threshold07=('imagenetV2', 'threshold0.7'),\n",
    "        imagenetV2_matched_frequency=('imagenetV2', 'matched-frequency')\n",
    "    )\n",
    "\n",
    "    def compute_metrics(self, df):\n",
    "        raise NotImplementedError(\"Subclasses of ModelAssay should implement `compute_metrics`.\")\n",
    "        \n",
    "    def plot_results(self, df):\n",
    "        raise NotImplementedError(\"Subclasses of ModelAssay should implement `plot_results`.\")\n",
    "    \n",
    "    def __call__(self, model_or_model_loader, transform):\n",
    "        self.dataloader = self.get_dataloader(transform)        \n",
    "        \n",
    "        if isinstance(model_or_model_loader, nn.Module):\n",
    "            model = model_or_model_loader\n",
    "        else:\n",
    "            model = model_or_model_loader()\n",
    "\n",
    "        df = validate(model, self.dataloader)\n",
    "        df['model_name'] = model.__dict__.get(\"model_name\", model.__class__.__name__)\n",
    "        df['dataset'] = self.dataset_name\n",
    "\n",
    "        # Clear the cache\n",
    "        del model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "            \n",
    "        return df\n",
    "     \n",
    "@torch.no_grad()        \n",
    "def validate(model, val_loader, print_freq=100, mb=None, store_outputs=False, set_eval=True):\n",
    "    if set_eval:\n",
    "        model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    filepaths = [(os.path.sep).join(f.split(os.path.sep)[-2:]) for f,_ in val_loader.dataset.imgs]\n",
    "    \n",
    "    results = defaultdict(list)\n",
    "    count = 0\n",
    "    for i, batch in enumerate(progress_bar(val_loader, parent=mb)):\n",
    "        batch_size = batch[0].shape[0]\n",
    "        images = batch[0].to(device, non_blocking=True)\n",
    "        target = batch[1].to(device, non_blocking=True)\n",
    "        index = batch[2].tolist()\n",
    "        filenames = [filepaths[idx] for idx in index]\n",
    "        \n",
    "        with autocast():\n",
    "            output = model(images)\n",
    "        loss = criterion(output, target)\n",
    "        preds, correct1, correct5 = accuracy(output, target, topk=(1, 5))\n",
    "\n",
    "        results['index'] += index\n",
    "        results['filenames'] += filenames\n",
    "        results['correct_label'] += target.tolist()\n",
    "        results['pred_label'] += preds[0].tolist()\n",
    "        results['loss'] += loss.tolist()\n",
    "        results['correct1'] += correct1.tolist()\n",
    "        results['correct5'] += correct5.tolist()\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    return df\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        corrects = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            corrects.append(correct[:k].any(dim=0).reshape(-1).float())\n",
    "        return pred, *corrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b78ed-ec76-4f65-85a4-ba463b84b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.alexnet(weights='IMAGENET1K_V1')\n",
    "# model = models.resnet50(weights='IMAGENET1K_V1')\n",
    "# model = models.resnet50(weights='IMAGENET1K_V1')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74052e27-9a68-41ba-bab6-073b25bdba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc3db46-62f0-4ecb-9861-232e5a3220f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import timm\n",
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# # timm.list_models(pretrained=True)\n",
    "\n",
    "# model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430dfdd6-4db3-438d-b83d-6a8000d594cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f280fa4a-f55f-4c4d-a8fd-b137f0d388a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     images = np.random.randint(0, 255, (500, 500, 3))\n",
    "#     inputs = processor(text=[\"\"], images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "#     outputs = model(**inputs)\n",
    "#     image_embeddings = outputs.image_embeds \n",
    "# image_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902e2aa1-1cba-41ed-8a25-f4f2e25a7e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from r3m import load_r3m\n",
    "# model = load_r3m(\"resnet50\") # resnet18, resnet34\n",
    "# model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f72659-306c-4d33-83e9-3d2074d20c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# # ENCODE IMAGE\n",
    "# image = np.random.randint(0, 255, (500, 500, 3))\n",
    "# preprocessed_image = transforms(Image.fromarray(image.astype(np.uint8))).reshape(-1, 3, 224, 224)\n",
    "# preprocessed_image.to(device) \n",
    "# with torch.no_grad():\n",
    "#     embedding = model(preprocessed_image * 255.0) ## R3M expects image input to be [0-255]\n",
    "# embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e51096-39cb-457f-87d1-5a656d3fec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms = transforms.Compose([\n",
    "#     transforms.Resize(256),\n",
    "#     transforms.CenterCrop(224),\n",
    "#     transforms.ToTensor(),\n",
    "#     lambda x: x * 255.0\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4edb4c-ca2f-4438-8739-b51568c6cf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_assay = ClassificationAccuracy(dataset='imagenetV2', split='matched-frequency')\n",
    "# cls_assay = ClassificationAccuracy(dataset='imagenet1k_s256', split='val')\n",
    "cls_assay.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3525e0dc-db40-4b10-91db-847cba5a6b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "img,label,index = cls_assay.dataset[0]\n",
    "print(index,label)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e908bcd9-2015-47a7-a192-eeebe03a2888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 63.15, 76.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c113535-baa1-4da6-886e-906ec102c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cls_assay(model, transform)\n",
    "results.correct1.mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f63cfc-eaa0-4a58-8dbf-fb66a5887b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_assay = ClassificationAccuracy(dataset='imagenet1k_s256', split='val')\n",
    "results2 = cls_assay(model, transform)\n",
    "results2.correct1.mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d753384b-0993-4f38-9432-2a7deb143601",
   "metadata": {},
   "outputs": [],
   "source": [
    "results2.correct1.mean() * 100 - results.correct1.mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a8fa1-95fb-481f-9842-fbae7817538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "model_name = 'vit_large_patch14_clip_224.openai_ft_in1k'\n",
    "model_name = 'vgg11.tv_in1k'\n",
    "# cfg = timm.get_pretrained_cfg(model_name).__dict__\n",
    "# cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52297e85-6aa9-4e60-a4a5-18ffc23add40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timm.list_models(pretrained=True)\n",
    "model = timm.create_model(model_name, pretrained=True)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ba444f-b9bb-43a7-b2a4-e84437663ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "# data_config['input_size'] = (3,224,224)\n",
    "# data_config['crop_pct'] = 1.0\n",
    "data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b29e3e8-3f59-4f07-ad32-97e612c166d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.data.create_transform(**data_config, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37170b45-9227-46dd-ad9e-50b3ce94c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = timm.data.create_transform(**data_config, is_training=False)\n",
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3a0d18-8ff1-4c1d-937c-4945c4268395",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "img_size = data_config['input_size'][-2:]\n",
    "with torch.no_grad():\n",
    "    x = torch.rand(10,3,*img_size).to(device)\n",
    "    output = model(x)\n",
    "x.shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032c18f2-0106-45c7-93e7-5bc526f13479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_size = cfg[\"test_input_size\"][-1] if \"test_input_size\" in cfg and cfg[\"test_input_size\"] else cfg[\"input_size\"][-1]\n",
    "# transform = timm.data.transforms_factory.transforms_imagenet_eval(\n",
    "#     img_size=img_size,\n",
    "#     interpolation=cfg[\"interpolation\"],\n",
    "#     mean=cfg[\"mean\"],\n",
    "#     std=cfg[\"std\"],\n",
    "#     crop_pct=cfg[\"crop_pct\"],\n",
    "#     crop_mode=cfg.get(\"crop_mode\", None)\n",
    "# )\n",
    "# transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae21bd5-568a-4cc2-b23a-9b0d0703577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize(256),\n",
    "#     transforms.CenterCrop(224),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "# transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6d9703-2560-46d6-8b19-ee0ebe02fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_assay = ClassificationAccuracy(dataset='imagenetV2', split='matched-frequency')\n",
    "# cls_assay = ClassificationAccuracy(dataset='imagenet1k_s256', split='val')\n",
    "cls_assay.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51174438-2b79-4443-8556-305326bff657",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cls_assay(model, transform)\n",
    "results.correct1.mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661f858-022c-4ee9-aacf-44bd2c8aae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_assay = ClassificationAccuracy(dataset='imagenet1k_s256', split='val')\n",
    "results2 = cls_assay(model, transform)\n",
    "results2.correct1.mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491637b6-3f52-4c88-920d-a0fba8ab22af",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.correct1.mean() * 100 - results2.correct1.mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5ef74c-24c5-47fd-bc85-148e56256a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -13.202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f4b84c-2586-48e1-85f0-faef7174b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf7668d-b41b-4939-88a3-5ab5466cc532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cleanvision import Imagelab\n",
    "\n",
    "# data_path = '/n/alvarez_lab_tier1/Lab/cache/torch/data/imagenetv2-matched-frequency-format-val-5fbc2174/imagenetv2-matched-frequency-format-val'\n",
    "# imagelab = Imagelab(data_path=data_path)\n",
    "\n",
    "# imagelab.find_issues()\n",
    "# imagelab.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e787eb-38c5-4384-9a23-6d9dbf25059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_assay = ClassificationAccuracy(dataset='imagenette2_s320_remap1k', split='val')\n",
    "res = cls_assay(model, transform)\n",
    "res.correct1.mean() * 100, res.correct5.mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caac419-4f84-4a8e-830a-0e682f6e7770",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.groupby(by=['correct_label']).correct1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16d776a-d0fc-414f-a061-9e5fcabae3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load the pretrained AlexNet model\n",
    "alexnet_weights = models.AlexNet_Weights.DEFAULT\n",
    "alexnet = models.alexnet(weights=alexnet_weights)\n",
    "\n",
    "# Get the transforms used for the pretrained model\n",
    "weights_transform = alexnet_weights.transforms()\n",
    "print(alexnet)\n",
    "print(weights_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d91ce-5fc9-41ea-b3a5-adae478f748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(weights_transform.crop_size, interpolation=weights_transform.interpolation),\n",
    "    transforms.CenterCrop(weights_transform.resize_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=weights_transform.mean,std=weights_transform.std)\n",
    "])\n",
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2c206-746b-41a1-838a-59cada071595",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
